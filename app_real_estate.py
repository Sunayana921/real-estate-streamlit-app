# -*- coding: utf-8 -*-
"""app real estate

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ti3dfcg0dgOY0VbBiB2u5uRjxsRHuy7R
"""




import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

st.set_page_config(page_title="Real Estate EDA Dashboard", layout="wide")

st.title("Real Estate Exploratory Data Analysis")

INPUT_PATH = "india_housing_prices.csv"
OUTPUT_PATH = "cleaned_data.csv"

# ---------------- LOAD DATA ----------------
df = pd.read_csv(INPUT_PATH)

st.write("### Original Dataset Shape")
st.write(df.shape)

st.write("### Initial Columns")
st.write(list(df.columns))

# ---------------- CLEANING ----------------
for col in df.select_dtypes(include="object"):
    df[col] = df[col].astype(str).str.strip().replace("nan", np.nan)

numeric_cols = [
    "BHK", "Size_in_SqFt", "Price_in_Lakhs", "Price_per_SqFt",
    "Year_Built", "Floor_No", "Total_Floors",
    "Age_of_Property", "Nearby_Schools", "Nearby_Hospitals",
    "Public_Transport_Accessibility", "Parking_Space"
]

for col in numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")

# ---------------- FEATURE FIXES ----------------
if "Price_in_Lakhs" in df.columns and "Size_in_SqFt" in df.columns:
    df["Price_Rs"] = df["Price_in_Lakhs"] * 100000
    df["Computed_PPSF"] = (df["Price_Rs"] / df["Size_in_SqFt"]).replace([np.inf, -np.inf], np.nan)
    if "Price_per_SqFt" not in df.columns:
        df["Price_per_SqFt"] = df["Computed_PPSF"]
    else:
        mask = df["Price_per_SqFt"].isna() | (df["Price_per_SqFt"] <= 0)
        df.loc[mask, "Price_per_SqFt"] = df.loc[mask, "Computed_PPSF"]

CURRENT_YEAR = 2025
if "Year_Built" in df.columns:
    missing_age = df["Age_of_Property"].isna()
    df.loc[missing_age, "Age_of_Property"] = CURRENT_YEAR - df.loc[missing_age, "Year_Built"]

# Fill missing values
for col in df.select_dtypes(include=[np.number]):
    df[col] = df[col].fillna(df[col].median())

for col in df.select_dtypes(include="object"):
    df[col] = df[col].fillna("Unknown")

# Remove duplicates
before = df.shape[0]
df = df.drop_duplicates()
after = df.shape[0]
st.write(f"Duplicates removed: {before - after}")

df.to_csv(OUTPUT_PATH, index=False)
st.success("Cleaned data saved successfully!")

# ---------------- SUMMARY ----------------
st.subheader("Dataset Summary")
st.dataframe(df.describe())

# ---------------- UNIVARIATE ANALYSIS ----------------
st.header("Univariate Analysis")

fig, ax = plt.subplots()
sns.histplot(df["Price_in_Lakhs"], bins=40, kde=True, ax=ax)
ax.set_title("Price Distribution (Lakhs)")
st.pyplot(fig)

fig, ax = plt.subplots()
sns.histplot(df["Size_in_SqFt"], bins=40, kde=True, ax=ax)
ax.set_title("Size Distribution (SqFt)")
st.pyplot(fig)

fig, ax = plt.subplots()
sns.histplot(df["Price_per_SqFt"], bins=40, kde=True, ax=ax)
ax.set_title("Price per SqFt Distribution")
st.pyplot(fig)

# ---------------- BIVARIATE ANALYSIS ----------------
st.header("Bivariate Analysis")

fig, ax = plt.subplots()
sns.scatterplot(x=df["Size_in_SqFt"], y=df["Price_in_Lakhs"], alpha=0.4, ax=ax)
ax.set_title("Size vs Price")
st.pyplot(fig)

if "City" in df.columns:
    fig, ax = plt.subplots(figsize=(10,4))
    city_avg = df.groupby("City")["Price_in_Lakhs"].mean().sort_values(ascending=False).head(10)
    sns.barplot(x=city_avg.index, y=city_avg.values, ax=ax)
    ax.set_title("Average Property Price by City (Top 10)")
    ax.tick_params(axis="x", rotation=45)
    st.pyplot(fig)

if "Property_Type" in df.columns:
    fig, ax = plt.subplots(figsize=(8,4))
    sns.boxplot(data=df, x="Property_Type", y="Price_per_SqFt", ax=ax)
    ax.set_title("Price per SqFt by Property Type")
    ax.tick_params(axis="x", rotation=45)
    st.pyplot(fig)

# ---------------- CORRELATION ----------------
st.header("Correlation Heatmap")

fig, ax = plt.subplots(figsize=(10,5))
sns.heatmap(df.select_dtypes(include=[np.number]).corr(), cmap="coolwarm", ax=ax)
st.pyplot(fig)

# ---------------- TOP LOCALITIES ----------------
if "Locality" in df.columns:
    st.header("Top 10 Most Expensive Localities")

    top_local = (
        df.groupby("Locality")["Price_per_SqFt"]
        .mean()
        .sort_values(ascending=False)
        .head(10)
    )

    fig, ax = plt.subplots()
sns.barplot(x=top_local.index, y=top_local.values, ax=ax)
ax.set_title("Top 10 Expensive Localities (Price/SqFt)")
ax.tick_params(axis="x", rotation=45)
st.pyplot(fig)

st.success("EDA Completed Successfully")

# full_real_estate_pipeline_fixed.py
"""
Corrected full pipeline:
 - robust preprocessing
 - improved regression target (CAGR + realistic noise)
 - multi-factor Good Investment label (with fallback to ensure >1 class)
 - train LinearRegression and LogisticRegression with pipelines
 - evaluation & model saving
"""

import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import (
    accuracy_score, f1_score, confusion_matrix, classification_report,
    mean_squared_error, mean_absolute_error, r2_score
)
import joblib

# -------------------------
# Config
# -------------------------
INPUT_CSV = "india_housing_prices.csv"      # change path if needed
CLEANED_CSV = "cleaned_data_fixed.csv"
CLASS_MODEL_PATH = "logistic_regression_model_fixed.pkl"
REG_MODEL_PATH = "linear_regression_model_fixed.pkl"

RANDOM_SEED = 42
ANNUAL_GROWTH = 0.08       # base CAGR used to create synthetic future price
REG_NOISE_PCT = 0.10       # std dev of noise as % of current price (10%)
INVESTMENT_SCORE_THRESHOLD = 2  # default threshold (will adjust if degenerate)

np.random.seed(RANDOM_SEED)


# -------------------------
# 1) LOAD
# -------------------------
df = pd.read_csv(INPUT_CSV)
print("Loaded raw dataset:", df.shape)

# -------------------------
# 2) BASIC CLEAN & TYPE FIXES
# -------------------------
# strip whitespace from string columns
for c in df.select_dtypes(include="object").columns:
    df[c] = df[c].astype(str).str.strip().replace({"nan": None})

# ensure numeric columns (coerce errors)
numeric_expected = [
    "BHK","Size_in_SqFt","Price_in_Lakhs","Price_per_SqFt",
    "Year_Built","Floor_No","Total_Floors","Age_of_Property",
    "Nearby_Schools","Nearby_Hospitals","Public_Transport_Accessibility","Parking_Space"
]
for c in numeric_expected:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# compute Price_Rs
if "Price_in_Lakhs" in df.columns:
    df["Price_Rs"] = df["Price_in_Lakhs"] * 100000
else:
    raise RuntimeError("Price_in_Lakhs column is required")

# compute Price_per_SqFt when missing
if "Price_per_SqFt" not in df.columns:
    df["Price_per_SqFt"] = df["Price_Rs"] / df["Size_in_SqFt"]
else:
    missing_ppsf = df["Price_per_SqFt"].isna() | (df["Price_per_SqFt"] <= 0)
    df.loc[missing_ppsf, "Price_per_SqFt"] = (df.loc[missing_ppsf, "Price_Rs"] / df.loc[missing_ppsf, "Size_in_SqFt"]).replace([np.inf, -np.inf], np.nan)

# Age_of_Property if missing and Year_Built exists
CURRENT_YEAR = 2025
if "Age_of_Property" not in df.columns or df["Age_of_Property"].isna().all():
    if "Year_Built" in df.columns:
        df["Age_of_Property"] = CURRENT_YEAR - df["Year_Built"]
    else:
        df["Age_of_Property"] = np.nan

# -------------------------
# 3) MISSING VALUE STRATEGY
# -------------------------
# Numeric: fill with median, or 0 if all NaNs
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
for c in num_cols:
    col_median = df[c].median()
    if pd.isna(col_median): # If the column is entirely NaN, median will be NaN
        df[c] = df[c].fillna(0) # Fill with 0 (assuming 0 means absence/default for these types of features)
    else:
        df[c] = df[c].fillna(col_median)

# Categorical: fill with "Unknown"
for c in df.select_dtypes(include=["object"]).columns:
    df[c] = df[c].fillna("Unknown")

# Deduplicate
df = df.drop_duplicates().reset_index(drop=True)
print("After cleaning & dedup: ", df.shape)

# -------------------------
# 4) FEATURE ENGINEERING
# -------------------------
# Amenity_Count from Amenities textual column (if exists)
if "Amenities" in df.columns:
    def amenity_count(x):
        if pd.isna(x) or str(x).strip().lower() in ["nan","none","unknown",""]:
            return 0
        parts = [p.strip() for p in str(x).split(",") if p.strip()]
        return len(parts)
    df["Amenity_Count"] = df["Amenities"].apply(amenity_count)
else:
    df["Amenity_Count"] = 0

# Social infra score (Hospitals weighted more + public transport inverted if encoded as distance)
if "Nearby_Hospitals" not in df.columns:
    df["Nearby_Hospitals"] = 0
if "Nearby_Schools" not in df.columns:
    df["Nearby_Schools"] = 0
if "Public_Transport_Accessibility" not in df.columns:
    # if no column, assume good by default (0 distance)
    df["Public_Transport_Accessibility"] = 0

def social_score(row):
    # Hospitals more valuable: weight 2
    hosp = float(row["Nearby_Hospitals"])
    sch = float(row["Nearby_Schools"])
    # If Public_Transport_Accessibility is distance-like (lower is better), invert: higher score = better
    pta = float(row["Public_Transport_Accessibility"])
    # If the column was already normalized accessibility (higher better), treat accordingly:
    # We'll assume column is distance in km or similar; invert with a cap (10)
    pta_score = max(0.0, 10.0 - pta)  # 10 - distance
    return hosp * 2.0 + sch + pta_score

df["social_infra_score"] = df.apply(social_score, axis=1)
# normalize to [0,1] (min-max)
min_s = df["social_infra_score"].min()
max_s = df["social_infra_score"].max()
if max_s - min_s > 0:
    df["social_infra_score_norm"] = (df["social_infra_score"] - min_s) / (max_s - min_s)
else:
    df["social_infra_score_norm"] = 0.0

# Furnished flag
if "Furnished_Status" in df.columns:
    df["is_furnished"] = df["Furnished_Status"].astype(str).str.lower().isin(
        ["fully","fully furnished","furnished","full"]).astype(int)
else:
    df["is_furnished"] = 0

# is_builder flag
if "Owner_Type" in df.columns:
    df["is_builder"] = df["Owner_Type"].astype(str).str.lower().str.contains("builder|developer|company|firm").astype(int)
else:
    df["is_builder"] = 0

# -------------------------
# 5) TARGET CREATION
# -------------------------
# 5.1 Regression target: realistic future price = deterministic CAGR + multiplicative noise
# base_future = current * (1+g)^5
base_future = df["Price_in_Lakhs"] * ((1.0 + ANNUAL_GROWTH) ** 5)

# Add multiplicative noise proportional to current price (10% std by default)
noise = np.random.normal(loc=0.0, scale=REG_NOISE_PCT, size=len(df))
# ensure noise factor is positive around 1.0: future = base * (1 + noise)
df["future_price_5yr_lakhs"] = (base_future * (1.0 + noise)).clip(lower=0.01)

# 5.2 Classification target: multi-factor investment score
# local median price_per_sqft
df["locality_median_ppsf"] = df.groupby("Locality")["Price_per_SqFt"].transform("median")
df["city_median_ppsf"] = df.groupby("City")["Price_per_SqFt"].transform("median")

# criteria (binary)
df["below_locality_avg"] = (df["Price_per_SqFt"] <= df["locality_median_ppsf"]).astype(int)
df["below_city_avg"] = (df["Price_per_SqFt"] <= df["city_median_ppsf"]).astype(int)
df["infra_good"] = (df["social_infra_score_norm"] >= df["social_infra_score_norm"].median()).astype(int)
df["bhk_good"] = (df["BHK"] >= 2).astype(int)
# amenity flag: more than median amenities
df["amenity_good"] = (df["Amenity_Count"] >= df["Amenity_Count"].median()).astype(int)

# composite score
df["investment_score"] = (
    df["below_locality_avg"]
    + df["below_city_avg"]
    + df["infra_good"]
    + df["bhk_good"]
    + df["amenity_good"]
)

# default threshold (>= 3 of 5 criteria)
threshold = INVESTMENT_SCORE_THRESHOLD
df["good_investment"] = (df["investment_score"] >= threshold).astype(int)

# If label is degenerate (only one class), adjust threshold automatically:
label_counts = df["good_investment"].value_counts()
if len(label_counts) == 1:
    # choose threshold as median(investment_score)
    auto_thresh = int(df["investment_score"].median().round())
    # ensure it's within 0..max score
    auto_thresh = max(0, min(auto_thresh, int(df["investment_score"].max())))
    # apply new threshold
    df["good_investment"] = (df["investment_score"] >= auto_thresh).astype(int)
    print(f"Warning: initial good_investment had 1 class. Auto-adjusted threshold to {auto_thresh}.")

# Re-check label distribution
print("good_investment label distribution:")
print(df["good_investment"].value_counts())

# -------------------------
# 6) SAVE CLEANED FILE
# -------------------------
df.to_csv(CLEANED_CSV, index=False)
print("Saved cleaned dataset:", CLEANED_CSV)

# -------------------------
# 7) MODELING PREPARATION
# -------------------------
# Features selection (keep a practical set)
features = [
    "Price_in_Lakhs", "Size_in_SqFt", "Price_per_SqFt", "BHK", "Age_of_Property",
    "Nearby_Schools", "Nearby_Hospitals", "Parking_Space",
    "Amenity_Count", "social_infra_score_norm", "is_furnished", "is_builder",
    "City", "Locality", "Property_Type", "Furnished_Status"
]
features = [c for c in features if c in df.columns]

X = df[features].copy()
y_reg = df["future_price_5yr_lakhs"].copy()
y_class = df["good_investment"].copy()

# Identify numeric/categorical
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=["object"]).columns.tolist()

print("Numeric features:", numeric_features)
print("Categorical features (sample):", categorical_features[:5])

# -------------------------
# 8) PIPELINE & TRAIN/TEST SPLIT
# -------------------------
preproc_num = Pipeline(steps=[("scaler", StandardScaler())])
preproc_cat = Pipeline(steps=[("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))])

preprocessor = ColumnTransformer(transformers=[
    ("num", preproc_num, numeric_features),
    ("cat", preproc_cat, categorical_features)
])

# regression split
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X, y_reg, test_size=0.2, random_state=RANDOM_SEED
)

# classification split (stratify)
if y_class.nunique() > 1:
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X, y_class, test_size=0.2, random_state=RANDOM_SEED, stratify=y_class
    )
else:
    # fallback: no stratify if single class (shouldn't happen due to earlier fix)
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X, y_class, test_size=0.2, random_state=RANDOM_SEED
    )

# -------------------------
# 9) REGRESSION MODEL (LinearRegression)
# -------------------------
reg_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", LinearRegression())
])

reg_pipeline.fit(X_train_r, y_train_r)
y_pred_r = reg_pipeline.predict(X_test_r)

rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_r))
mae = mean_absolute_error(y_test_r, y_pred_r)
r2 = r2_score(y_test_r, y_pred_r)

print("\nREGRESSION EVALUATION")
print(f"RMSE: {rmse:.4f}")
print(f"MAE:  {mae:.4f}")
print(f"R2:   {r2:.4f}")

# Save regression model
joblib.dump(reg_pipeline, REG_MODEL_PATH)
print("Saved regression model to:", REG_MODEL_PATH)

# -------------------------
# 10) CLASSIFICATION MODEL (LogisticRegression)
# -------------------------
clf_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression(max_iter=2000, random_state=RANDOM_SEED))
])

# Train
clf_pipeline.fit(X_train_c, y_train_c)
y_pred_c = clf_pipeline.predict(X_test_c)

acc = accuracy_score(y_test_c, y_pred_c)
f1 = f1_score(y_test_c, y_pred_c, zero_division=0)

print("\nCLASSIFICATION EVALUATION")
print(f"Accuracy: {acc:.4f}")
print(f"F1-score: {f1:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test_c, y_pred_c))
print("\nClassification Report:\n", classification_report(y_test_c, y_pred_c))

# Save classification model
joblib.dump(clf_pipeline, CLASS_MODEL_PATH)
print("Saved classification model to:", CLASS_MODEL_PATH)

print("\nPipeline complete. Models and cleaned dataset saved.")
